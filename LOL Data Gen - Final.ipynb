{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45681cdf-c0b6-4dfb-b8a7-c64b402c1513",
   "metadata": {},
   "source": [
    "# **Initialize API Client and Directories**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a5ab6-d46c-483d-bcbd-10409ac60c28",
   "metadata": {},
   "source": [
    "Loads environment variables, initializes the RiotWatcher client with the API key, and prepares folders for storing raw and processed game data. Sets constants for region, game mode, and the time period for collecting ranked solo queue matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd71f184-6e90-45dc-9830-7699ad0fa9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from riotwatcher import LolWatcher, ApiError\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "API_KEY = \"RGAPI-99fc4ea1-5b13-49f4-9aa8-0acfa99546b7\"\n",
    "assert API_KEY, \"Please set RIOT_API_KEY in your .env file.\"\n",
    "\n",
    "watcher = LolWatcher(API_KEY)\n",
    "REGION = \"europe\"\n",
    "PLATFORM = \"euw1\"\n",
    "QUEUE_ID = 420  # Ranked Solo\n",
    "START = int(datetime(2024,1,1,tzinfo=timezone.utc).timestamp())\n",
    "END   = int(datetime(2025,12,31,23,59,59,tzinfo=timezone.utc).timestamp())\n",
    "DATA_DIR = Path(\"data\")\n",
    "RAW_MATCH_DIR = DATA_DIR / \"raw/matches\"\n",
    "RAW_TL_DIR = DATA_DIR / \"raw/timelines\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "for d in [RAW_MATCH_DIR, RAW_TL_DIR, PROC_DIR]: d.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8416d93e-d363-4242-abfc-0fd2b7320b12",
   "metadata": {},
   "source": [
    "# **Retrieve Top Player PUUIDs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec14aea3-094c-4d15-8bad-3b12fbe96f25",
   "metadata": {},
   "source": [
    "Fetches player unique identifiers (PUUIDs) from the challenger, grandmaster, or master ranked ladders for the European server. Limits the number of PUUIDs collected and ensures uniqueness, providing a high-skill player base for subsequent match data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ee1f9a-bcd4-4f08-ba72-3a3d9b819e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seed_puuids(limit=200):\n",
    "    \"\"\"\n",
    "    Fetch challenger (or fallback grandmaster/master) ladder entries and extract PUUIDs directly.\n",
    "    Compatible with RiotWatcher versions returning 'puuid' inside ladder entries.\n",
    "    \"\"\"\n",
    "    tiers = [\n",
    "        (\"challenger_by_queue\", watcher.league.challenger_by_queue),\n",
    "        (\"grandmaster_by_queue\", watcher.league.grandmaster_by_queue),\n",
    "        (\"masters_by_queue\", watcher.league.masters_by_queue)\n",
    "    ]\n",
    "\n",
    "    entries = []\n",
    "    for name, func in tiers:\n",
    "        try:\n",
    "            data = func(\"euw1\", \"RANKED_SOLO_5x5\")\n",
    "            ents = data.get(\"entries\", [])\n",
    "            if ents:\n",
    "                print(f\"Found {len(ents)} entries in {name}\")\n",
    "                entries = ents\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Failed {name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not entries:\n",
    "        print(\"No ladder entries found — check API key or region.\")\n",
    "        return []\n",
    "\n",
    "    puuids = [e[\"puuid\"] for e in entries[:limit] if \"puuid\" in e]\n",
    "    puuids = list(dict.fromkeys(puuids))\n",
    "\n",
    "    print(f\"Collected {len(puuids)} unique PUUIDs directly from ladder\")\n",
    "    return puuids\n",
    "\n",
    "\n",
    "seed_puuids = get_seed_puuids(100)\n",
    "len(seed_puuids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db1065-3440-4b18-9515-0d8c301a0d03",
   "metadata": {},
   "source": [
    "# **Collect Match IDs for Seed Players**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a685e3b3-d7c0-4735-a30d-44011d1c5d40",
   "metadata": {},
   "source": [
    "Loops through each player's PUUID to retrieve match IDs from ranked solo queue games within the targeted time period. Handles API rate limits and errors, consolidates unique match references, and saves them to a CSV for later downloading of full match data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764935c-61df-4fa1-843f-f6f7ea2780bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from riotwatcher import ApiError\n",
    "\n",
    "DATA_DIR = Path(\"data/interim\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "QUEUE_ID = 420 \n",
    "START = int(datetime(2024, 1, 1, tzinfo=timezone.utc).timestamp())\n",
    "END   = int(datetime(2025, 12, 31, 23, 59, 59, tzinfo=timezone.utc).timestamp())\n",
    "\n",
    "def get_match_ids(puuids, pages=2, per_page=100):\n",
    "    \"\"\"\n",
    "    Fetch recent match IDs for each player PUUID.\n",
    "    \"\"\"\n",
    "    recs = []\n",
    "\n",
    "    for puuid in tqdm(puuids, desc=\"Fetching match IDs\"):\n",
    "        for p in range(pages):\n",
    "            try:\n",
    "                mids = watcher._match.matchlist_by_puuid(\n",
    "                    \"europe\", \n",
    "                    puuid,\n",
    "                    start=p * per_page,\n",
    "                    count=per_page,\n",
    "                    queue=QUEUE_ID,\n",
    "                    start_time=START,\n",
    "                    end_time=END\n",
    "                )\n",
    "                if not mids:\n",
    "                    break\n",
    "                recs += [{\"puuid\": puuid, \"match_id\": mid} for mid in mids]\n",
    "                time.sleep(0.1)\n",
    "            except ApiError as e:\n",
    "                if getattr(e.response, \"status_code\", None) == 429:\n",
    "                    print(\"Rate limit hit, sleeping 2s...\")\n",
    "                    time.sleep(2)\n",
    "                break\n",
    "            except Exception as ex:\n",
    "                print(f\"Error for {puuid}: {ex}\")\n",
    "                break\n",
    "\n",
    "    df = pd.DataFrame(recs).drop_duplicates()\n",
    "    df.to_csv(DATA_DIR / \"match_ids.csv\", index=False)\n",
    "    print(f\"Saved {len(df)} match records to {DATA_DIR / 'match_ids.csv'}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "match_ids_df = get_match_ids(seed_puuids, pages=3)\n",
    "len(match_ids_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e7571-57b9-4999-aa4c-8fabbfab7bf8",
   "metadata": {},
   "source": [
    "# **Download Match and Timeline Data with Retry**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12626772-b197-4a5b-9eb7-38f538d5bb3b",
   "metadata": {},
   "source": [
    "Fetches full match details and timelines for each match ID, storing them as JSON files with caching to avoid redundant downloads. Implements automatic retries with exponential backoff to handle API rate limits and failures, ensuring robust data collection for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa59f6b-337e-4512-b349-a8bb5e07b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from riotwatcher import ApiError\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "\n",
    "RAW_MATCH_DIR = Path(\"data/raw/matches\")\n",
    "RAW_TL_DIR = Path(\"data/raw/timelines\")\n",
    "RAW_MATCH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RAW_TL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "@retry(retry=retry_if_exception_type(ApiError), wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5), reraise=True)\n",
    "def fetch_match(mid):\n",
    "    \"\"\"Fetch match details using the new RiotWatcher structure.\"\"\"\n",
    "    return watcher._match.by_id(\"europe\", mid)\n",
    "\n",
    "@retry(retry=retry_if_exception_type(ApiError), wait=wait_exponential(min=1, max=60), stop=stop_after_attempt(5), reraise=True)\n",
    "def fetch_timeline(mid):\n",
    "    \"\"\"Fetch timeline data using the new RiotWatcher structure.\"\"\"\n",
    "    return watcher._match.timeline_by_match(\"europe\", mid)\n",
    "\n",
    "def download_all(match_ids):\n",
    "    \"\"\"\n",
    "    Download all matches and timelines, with retry + caching.\n",
    "    \"\"\"\n",
    "    ok_m = ok_tl = miss_tl = 0\n",
    "    for mid in tqdm(match_ids, desc=\"Downloading matches & timelines\"):\n",
    "        m_path = RAW_MATCH_DIR / f\"{mid}.json\"\n",
    "        tl_path = RAW_TL_DIR / f\"{mid}.json\"\n",
    "\n",
    "        # Match JSON\n",
    "        if not m_path.exists():\n",
    "            try:\n",
    "                m_data = fetch_match(mid)\n",
    "                m_path.write_text(json.dumps(m_data))\n",
    "                ok_m += 1\n",
    "            except ApiError as e:\n",
    "                print(f\"Match fetch failed for {mid}: {getattr(e.response, 'status_code', '?')}\")\n",
    "                time.sleep(2)\n",
    "\n",
    "        \n",
    "        if not tl_path.exists():\n",
    "            try:\n",
    "                tl_data = fetch_timeline(mid)\n",
    "                tl_path.write_text(json.dumps(tl_data))\n",
    "                ok_tl += 1\n",
    "            except ApiError:\n",
    "                miss_tl += 1\n",
    "                continue\n",
    "\n",
    "        time.sleep(0.25) \n",
    "\n",
    "    print(f\"\\nDownload complete — Matches: {ok_m}, Timelines: {ok_tl}, Missed: {miss_tl}\")\n",
    "    return ok_m, ok_tl, miss_tl\n",
    "\n",
    "\n",
    "# ⚙️ Run in batches (to avoid hitting the rate limit hard)\n",
    "mids = match_ids_df[\"match_id\"].unique().tolist()\n",
    "download_all(mids[:500])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca16616-2808-4775-9f95-f88d93d69fe2",
   "metadata": {},
   "source": [
    "# **Extract Player Stats from Matches**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d611f4-2961-4a74-b8a3-dafb31b03852",
   "metadata": {},
   "source": [
    "Reads all raw match JSON files and converts detailed player statistics into a single flat table. Includes key stats like kills, deaths, damage, healing, and objectives to form the basis for player performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb2439-fa1a-4cb8-83e1-4d7c6c184653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "RAW_MATCH_DIR = Path(\"data/raw/matches\")\n",
    "OUT = Path(\"data/processed/player_stats.csv\")\n",
    "OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def flatten_matches():\n",
    "    \"\"\"\n",
    "    Flatten all match JSONs into player-level records.\n",
    "    Compatible with RiotWatcher v3.3+ and your match JSON schema.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for p in tqdm(list(RAW_MATCH_DIR.glob(\"*.json\")), desc=\"Flattening matches\"):\n",
    "        try:\n",
    "            m = json.loads(p.read_text())\n",
    "            info = m.get(\"info\", {})\n",
    "            participants = info.get(\"participants\", [])\n",
    "            if not participants:\n",
    "                continue\n",
    "\n",
    "            for pl in participants:\n",
    "                rows.append({\n",
    "                    \"match_id\": m.get(\"metadata\", {}).get(\"matchId\"),\n",
    "                    \"id\": pl.get(\"puuid\"),\n",
    "                    \"win\": int(pl.get(\"win\", 0)),\n",
    "                    \"kills\": pl.get(\"kills\", 0),\n",
    "                    \"deaths\": pl.get(\"deaths\", 0),\n",
    "                    \"assists\": pl.get(\"assists\", 0),\n",
    "                    \"largestkillingspree\": pl.get(\"largestKillingSpree\", 0),\n",
    "                    \"largestmultikill\": pl.get(\"largestMultiKill\", 0),\n",
    "                    \"longesttimespentliving\": pl.get(\"longestTimeSpentLiving\", 0),\n",
    "                    \"doublekills\": pl.get(\"doubleKills\", 0),\n",
    "                    \"triplekills\": pl.get(\"tripleKills\", 0),\n",
    "                    \"quadrakills\": pl.get(\"quadraKills\", 0),\n",
    "                    \"pentakills\": pl.get(\"pentaKills\", 0),\n",
    "                    \"totdmgdealt\": pl.get(\"totalDamageDealt\", 0),\n",
    "                    \"magicdmgdealt\": pl.get(\"magicDamageDealt\", 0),\n",
    "                    \"physicaldmgdealt\": pl.get(\"physicalDamageDealt\", 0),\n",
    "                    \"truedmgdealt\": pl.get(\"trueDamageDealt\", 0),\n",
    "                    \"largestcrit\": pl.get(\"largestCriticalStrike\", 0),\n",
    "                    \"totdmgtochamp\": pl.get(\"totalDamageDealtToChampions\", 0),\n",
    "                    \"magicdmgtochamp\": pl.get(\"magicDamageDealtToChampions\", 0),\n",
    "                    \"physdmgtochamp\": pl.get(\"physicalDamageDealtToChampions\", 0),\n",
    "                    \"truedmgtochamp\": pl.get(\"trueDamageDealtToChampions\", 0),\n",
    "                    \"totheal\": pl.get(\"totalHeal\", 0),\n",
    "                    \"totunitshealed\": pl.get(\"totalUnitsHealed\", 0),\n",
    "                    \"dmgtoturrets\": pl.get(\"damageDealtToTurrets\", 0),\n",
    "                    \"timecc\": pl.get(\"timeCCingOthers\", 0),\n",
    "                    \"totdmgtaken\": pl.get(\"totalDamageTaken\", 0),\n",
    "                    \"magicdmgtaken\": pl.get(\"magicDamageTaken\", 0),\n",
    "                    \"physdmgtaken\": pl.get(\"physicalDamageTaken\", 0),\n",
    "                    \"truedmgtaken\": pl.get(\"trueDamageTaken\", 0),\n",
    "                    \"wardsplaced\": pl.get(\"wardsPlaced\", 0),\n",
    "                    \"wardskilled\": pl.get(\"wardsKilled\", 0),\n",
    "                    \"firstblood\": int(pl.get(\"firstBloodKill\", False))\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {p.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(OUT, index=False)\n",
    "    print(f\"\\nFlattened {len(df)} player records → {OUT}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "player_df = flatten_matches()\n",
    "len(player_df), player_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d631d49-b458-4558-a8dd-449ab5e5b338",
   "metadata": {},
   "source": [
    "# **Extract and Flatten Match Events**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557a74e2-8a77-49e7-a752-3b7993a9c209",
   "metadata": {},
   "source": [
    "Processes timeline JSON files to collect all in-game events, such as kills, objectives, and item purchases, into a simple table with timestamps. This event-level data captures the flow of key actions across matches, forming the foundation for event prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67a31c0-4b09-434a-998a-a878b5188749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "RAW_TL_DIR = Path(\"data/raw/timelines\")\n",
    "OUT = Path(\"data/processed/events.csv\")\n",
    "\n",
    "def flatten_events(mid, tl):\n",
    "    rows = []\n",
    "    for fr in tl.get(\"info\", {}).get(\"frames\", []):\n",
    "        t = int((fr.get(\"timestamp\") or 0) // 1000)\n",
    "        for ev in fr.get(\"events\", []):\n",
    "            rows.append({\n",
    "                \"match_id\": mid,\n",
    "                \"t\": int((ev.get(\"timestamp\") or t) // 1000),\n",
    "                \"type\": ev.get(\"type\"),\n",
    "                \"teamId\": ev.get(\"teamId\"),\n",
    "                \"killerId\": ev.get(\"killerId\"),\n",
    "                \"victimId\": ev.get(\"victimId\"),\n",
    "                \"monsterType\": ev.get(\"monsterType\"),\n",
    "                \"buildingType\": ev.get(\"buildingType\"),\n",
    "                \"laneType\": ev.get(\"laneType\"),\n",
    "                \"itemId\": ev.get(\"itemId\")\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "\n",
    "def process_events():\n",
    "    rows = []\n",
    "    for p in tqdm(RAW_TL_DIR.glob(\"*.json\"), desc=\"Flattening events\"):\n",
    "        tl = json.loads(p.read_text())\n",
    "        mid = tl.get(\"metadata\", {}).get(\"matchId\")\n",
    "        if not mid:\n",
    "            continue\n",
    "        rows += flatten_events(mid, tl)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(OUT, index=False)\n",
    "    print(f\"Flattened {len(df)} events → {OUT}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "events_df = process_events()\n",
    "len(events_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa48d3ef-907e-4375-95ef-f6c7044bb065",
   "metadata": {},
   "source": [
    "# **Generate 10-Second Interval Team Timelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d6fc9-c91a-43e4-8aaf-a7f4474e7ee7",
   "metadata": {},
   "source": [
    "Processes raw timeline data to create team-level snapshots every 10 seconds, summarizing gold, experience, kills, player health, positioning, and clustering. Calculates metrics like team grouping, distance between teams, and presence near major objectives, providing a detailed dynamic game state for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6278a8-d1c7-46e7-b23c-8ef860154364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "RAW_TL_DIR = Path(\"data/raw/timelines\")\n",
    "RAW_MATCH_DIR = Path(\"data/raw/matches\")\n",
    "OUT = Path(\"data/processed/timeline_10s.csv\")\n",
    "\n",
    "BARON_POS = (5000, 10500)\n",
    "DRAGON_POS = (9850, 4400)\n",
    "\n",
    "def safe_get(d, *keys, default=0):\n",
    "    \"\"\"Safe nested dictionary get.\"\"\"\n",
    "    for k in keys:\n",
    "        if not isinstance(d, dict) or k not in d:\n",
    "            return default\n",
    "        d = d[k]\n",
    "    return d\n",
    "\n",
    "def dist(a, b):\n",
    "    return math.dist(a, b)\n",
    "\n",
    "def near(pos, pts, r=2500):\n",
    "    return sum(dist(pos,p) < r for p in pts)\n",
    "\n",
    "def cluster_density(pts):\n",
    "    if len(pts) < 2: return 9999\n",
    "    xs = [p[0] for p in pts]\n",
    "    ys = [p[1] for p in pts]\n",
    "    return (max(xs)-min(xs)) + (max(ys)-min(ys))\n",
    "\n",
    "def grouping_score(pts):\n",
    "    if len(pts) < 2: return 0\n",
    "    return sum(\n",
    "        dist(pts[i], pts[j]) < 1500\n",
    "        for i in range(len(pts))\n",
    "        for j in range(i+1,len(pts))\n",
    "    )\n",
    "\n",
    "def timeline_10s(mid, tl, events_df, tick=10):\n",
    "    match = json.loads((RAW_MATCH_DIR / f\"{mid}.json\").read_text())\n",
    "    dur = int(match[\"info\"].get(\"gameDuration\", 0))\n",
    "    if dur <= 0:\n",
    "        return None\n",
    "\n",
    "    ts = np.arange(0, dur+1, tick)\n",
    "    df = pd.DataFrame({\"match_id\": mid, \"t\": ts})\n",
    "\n",
    "    float_cols = [\n",
    "        \"cluster_density\",\n",
    "        \"distance_between_teams\",\n",
    "        \"gold_blue\",\"gold_red\",\"xp_blue\",\"xp_red\",\n",
    "        \"gold_diff\",\"xp_diff\",\n",
    "        \"kills_blue\",\"kills_red\",\n",
    "        \"kills_30_t1\",\"kills_30_t2\",\"deaths_30_t1\",\"deaths_30_t2\",\n",
    "        \"group_t1\",\"group_t2\",\n",
    "        \"alive_t1\",\"alive_t2\",\"alive_diff\",\n",
    "        \"low_hp_t1\",\"low_hp_t2\",\n",
    "        \"champs_near_baron_t1\",\"champs_near_baron_t2\",\n",
    "        \"champs_near_dragon_t1\",\"champs_near_dragon_t2\"\n",
    "    ]\n",
    "    \n",
    "    for col in float_cols:\n",
    "        df[col] = df[col].astype(\"float64\")\n",
    "\n",
    "\n",
    "    \n",
    "    for col in [\n",
    "        \"kills_blue\",\"kills_red\",\"gold_blue\",\"gold_red\",\"xp_blue\",\"xp_red\",\n",
    "        \"alive_t1\",\"alive_t2\",\"low_hp_t1\",\"low_hp_t2\",\n",
    "        \"champs_near_baron_t1\",\"champs_near_baron_t2\",\n",
    "        \"champs_near_dragon_t1\",\"champs_near_dragon_t2\",\n",
    "        \"group_t1\",\"group_t2\",\"cluster_density\",\"distance_between_teams\",\n",
    "        \"kills_30_t1\",\"kills_30_t2\",\"deaths_30_t1\",\"deaths_30_t2\",\n",
    "        \"gold_diff\",\"xp_diff\",\"alive_diff\"\n",
    "    ]:\n",
    "        df[col] = 0\n",
    "\n",
    "   \n",
    "    all_events = events_df[events_df.match_id==mid]\n",
    "\n",
    "  \n",
    "    for fr in tl[\"info\"][\"frames\"]:\n",
    "        ft = int(fr.get(\"timestamp\", 0) / 1000)\n",
    "\n",
    "        pf = fr.get(\"participantFrames\", {})\n",
    "        if not pf:\n",
    "            continue\n",
    "            \n",
    "        blue_pf = [v for k,v in pf.items() if int(k)<=5]\n",
    "        red_pf  = [v for k,v in pf.items() if int(k)>5]\n",
    "\n",
    "        df.loc[df.t>=ft,\"gold_blue\"] = sum(safe_get(pf,str(i),\"totalGold\",default=0) for i in range(1,6))\n",
    "        df.loc[df.t>=ft,\"gold_red\"]  = sum(safe_get(pf,str(i),\"totalGold\",default=0) for i in range(6,11))\n",
    "        df.loc[df.t>=ft,\"xp_blue\"]   = sum(safe_get(pf,str(i),\"xp\",default=0) for i in range(1,6))\n",
    "        df.loc[df.t>=ft,\"xp_red\"]    = sum(safe_get(pf,str(i),\"xp\",default=0) for i in range(6,11))\n",
    "\n",
    "        positions = {}\n",
    "        for pid,p in pf.items():\n",
    "            pos = p.get(\"position\")\n",
    "            if pos:\n",
    "                positions[int(pid)] = (pos[\"x\"], pos[\"y\"])\n",
    "\n",
    "        t1_pts = [positions[i] for i in range(1,6) if i in positions]\n",
    "        t2_pts = [positions[i] for i in range(6,11) if i in positions]\n",
    "\n",
    "        \n",
    "        t1_alive = sum(safe_get(pf,str(i),\"championStats\",\"currentHealth\", default=0) > 0 for i in range(1,6))\n",
    "        t2_alive = sum(safe_get(pf,str(i),\"championStats\",\"currentHealth\", default=0) > 0 for i in range(6,11))\n",
    "\n",
    "        t1_lowhp = sum(safe_get(pf,str(i),\"championStats\",\"currentHealth\", default=0) < 200 for i in range(1,6))\n",
    "        t2_lowhp = sum(safe_get(pf,str(i),\"championStats\",\"currentHealth\", default=0) < 200 for i in range(6,11))\n",
    "\n",
    "        df.loc[df.t>=ft, \"alive_t1\"] = t1_alive\n",
    "        df.loc[df.t>=ft, \"alive_t2\"] = t2_alive\n",
    "        df.loc[df.t>=ft, \"low_hp_t1\"] = t1_lowhp\n",
    "        df.loc[df.t>=ft, \"low_hp_t2\"] = t2_lowhp\n",
    "\n",
    "        df.loc[df.t>=ft, \"champs_near_baron_t1\"] = near(BARON_POS, t1_pts)\n",
    "        df.loc[df.t>=ft, \"champs_near_baron_t2\"] = near(BARON_POS, t2_pts)\n",
    "        df.loc[df.t>=ft, \"champs_near_dragon_t1\"] = near(DRAGON_POS, t1_pts)\n",
    "        df.loc[df.t>=ft, \"champs_near_dragon_t2\"] = near(DRAGON_POS, t2_pts)\n",
    "\n",
    "        df.loc[df.t>=ft,\"group_t1\"] = grouping_score(t1_pts)\n",
    "        df.loc[df.t>=ft,\"group_t2\"] = grouping_score(t2_pts)\n",
    "\n",
    "        df.loc[df.t>=ft,\"cluster_density\"] = (cluster_density(t1_pts)+cluster_density(t2_pts))/2\n",
    "\n",
    "        if t1_pts and t2_pts:\n",
    "            t1c = np.mean(t1_pts,axis=0)\n",
    "            t2c = np.mean(t2_pts,axis=0)\n",
    "            df.loc[df.t>=ft,\"distance_between_teams\"] = dist(t1c,t2c)\n",
    "        else:\n",
    "            df.loc[df.t>=ft,\"distance_between_teams\"] = 8000\n",
    "\n",
    "    for _,e in all_events.iterrows():\n",
    "        if e.type==\"CHAMPION_KILL\":\n",
    "            side = \"blue\" if e.killerId and int(e.killerId)<=5 else \"red\"\n",
    "            df.loc[df.t>=e.t,f\"kills_{side}\"] += 1\n",
    "\n",
    "    for i,row in df.iterrows():\n",
    "        ts = row.t\n",
    "        recent = all_events[(all_events.t>=ts-30) & (all_events.t<=ts)]\n",
    "\n",
    "        df.at[i,\"kills_30_t1\"] = sum(ev.killerId and int(ev.killerId)<=5 for _,ev in recent.iterrows() if ev.type==\"CHAMPION_KILL\")\n",
    "        df.at[i,\"kills_30_t2\"] = sum(ev.killerId and int(ev.killerId)>5  for _,ev in recent.iterrows() if ev.type==\"CHAMPION_KILL\")\n",
    "\n",
    "        df.at[i,\"deaths_30_t1\"] = sum(ev.victimId and int(ev.victimId)<=5 for _,ev in recent.iterrows() if ev.type==\"CHAMPION_KILL\")\n",
    "        df.at[i,\"deaths_30_t2\"] = sum(ev.victimId and int(ev.victimId)>5  for _,ev in recent.iterrows() if ev.type==\"CHAMPION_KILL\")\n",
    "\n",
    "   \n",
    "    df[\"gold_diff\"] = df.gold_blue - df.gold_red\n",
    "    df[\"xp_diff\"]   = df.xp_blue   - df.xp_red\n",
    "    df[\"alive_diff\"] = df.alive_t1 - df.alive_t2\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_timelines(events_df):\n",
    "    parts = []\n",
    "    for p in tqdm(RAW_TL_DIR.glob(\"*.json\"), desc=\"Building timelines\"):\n",
    "        tl = json.loads(p.read_text())\n",
    "        mid = tl[\"metadata\"][\"matchId\"]\n",
    "        part = timeline_10s(mid, tl, events_df)\n",
    "        if part is not None:\n",
    "            parts.append(part)\n",
    "    df = pd.concat(parts, ignore_index=True)\n",
    "    df.to_csv(OUT, index=False)\n",
    "    print(\"Fixed timeline_10s saved\")\n",
    "    return df\n",
    "\n",
    "\n",
    "timeline_df = build_timelines(events_df)\n",
    "timeline_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b0e7c-9dce-4698-98dc-56336e3269e0",
   "metadata": {},
   "source": [
    "# **Label Future Objectives and Merge Team Stats**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfd9adf-694d-4718-aa2c-92371f2e806f",
   "metadata": {},
   "source": [
    "Marks upcoming objectives and fights within specific time windows and combines these labels with team-level player statistics. This produces a final comprehensive dataset ready for building predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb6aca-0aea-4e35-b613-afa7673155de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA = Path(\"data/processed\")\n",
    "\n",
    "timeline = pd.read_csv(DATA / \"timeline_10s.csv\")\n",
    "events   = pd.read_csv(DATA / \"events.csv\")\n",
    "player   = pd.read_csv(DATA / \"player_stats.csv\")\n",
    "\n",
    "def extract_objective_times(events):\n",
    "    obj = {}\n",
    "    for mid,df in events.groupby(\"match_id\"):\n",
    "        baron  = df[(df.type==\"ELITE_MONSTER_KILL\") & (df.monsterType==\"BARON_NASHOR\")][\"t\"].tolist()\n",
    "        dragon = df[(df.type==\"ELITE_MONSTER_KILL\") & (df.monsterType==\"DRAGON\")][\"t\"].tolist()\n",
    "\n",
    "        kills = df[df.type==\"CHAMPION_KILL\"][\"t\"].sort_values().tolist()\n",
    "        fights=[]\n",
    "        for i,t in enumerate(kills):\n",
    "            if sum(1 for x in kills if t <= x <= t+10) >=3:\n",
    "                fights.append(t)\n",
    "\n",
    "        obj[mid]={\"baron\":baron,\"dragon\":dragon,\"teamfight\":fights}\n",
    "    return obj\n",
    "\n",
    "objective_times = extract_objective_times(events)\n",
    "\n",
    "WINDOWS=[10,20,30]\n",
    "EVENTS=[\"baron\",\"dragon\",\"teamfight\"]\n",
    "\n",
    "def label_future(ts, event_list, w):\n",
    "    return int(any(ts < e <= ts+w for e in event_list))\n",
    "\n",
    "rows=[]\n",
    "for mid,df in timeline.groupby(\"match_id\"):\n",
    "    df=df.copy()\n",
    "    obj=objective_times.get(mid,{\"baron\":[],\"dragon\":[],\"teamfight\":[]})\n",
    "\n",
    "    for ev in EVENTS:\n",
    "        for w in WINDOWS:\n",
    "            df[f\"{ev}_{w}\"]=df[\"t\"].apply(lambda t: label_future(t,obj[ev],w))\n",
    "    rows.append(df)\n",
    "\n",
    "labeled_timeline = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "player[\"team\"]=player.groupby(\"match_id\").cumcount().apply(\n",
    "    lambda x:\"blue\" if x<5 else \"red\"\n",
    ")\n",
    "\n",
    "team_stats = (\n",
    "    player.groupby([\"match_id\",\"team\"])\n",
    "    .agg({\n",
    "        \"kills\":\"sum\",\n",
    "        \"deaths\":\"sum\",\n",
    "        \"assists\":\"sum\",\n",
    "        \"totdmgdealt\":\"mean\",\n",
    "        \"totdmgtochamp\":\"mean\",\n",
    "        \"totheal\":\"mean\",\n",
    "        \"wardsplaced\":\"mean\",\n",
    "        \"wardskilled\":\"mean\"\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "team_stats = team_stats.pivot(index=\"match_id\", columns=\"team\")\n",
    "team_stats.columns = [f\"{a}_{b}\" for a,b in team_stats.columns]\n",
    "team_stats.reset_index(inplace=True)\n",
    "\n",
    "final = (\n",
    "    labeled_timeline\n",
    "    .merge(team_stats,on=\"match_id\",how=\"left\")\n",
    "    .sort_values([\"match_id\",\"t\"])\n",
    ")\n",
    "\n",
    "final.to_csv(DATA / \"final_dataset.csv\", index=False)\n",
    "print(final.shape)\n",
    "final.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
